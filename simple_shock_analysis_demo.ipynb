{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: akshare in c:\\anaconda\\lib\\site-packages (1.16.60)\n",
      "Requirement already satisfied: aiohttp>=3.11.13 in c:\\anaconda\\lib\\site-packages (from akshare) (3.11.14)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9.1 in c:\\anaconda\\lib\\site-packages (from akshare) (4.12.2)\n",
      "Requirement already satisfied: lxml>=4.2.1 in c:\\anaconda\\lib\\site-packages (from akshare) (4.9.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\anaconda\\lib\\site-packages (from akshare) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\anaconda\\lib\\site-packages (from akshare) (2.31.0)\n",
      "Requirement already satisfied: html5lib>=1.0.1 in c:\\anaconda\\lib\\site-packages (from akshare) (1.1)\n",
      "Requirement already satisfied: xlrd>=1.2.0 in c:\\anaconda\\lib\\site-packages (from akshare) (2.0.1)\n",
      "Requirement already satisfied: urllib3>=1.25.8 in c:\\anaconda\\lib\\site-packages (from akshare) (2.0.7)\n",
      "Requirement already satisfied: tqdm>=4.43.0 in c:\\anaconda\\lib\\site-packages (from akshare) (4.65.0)\n",
      "Requirement already satisfied: openpyxl>=3.0.3 in c:\\anaconda\\lib\\site-packages (from akshare) (3.0.10)\n",
      "Requirement already satisfied: jsonpath>=0.82 in c:\\anaconda\\lib\\site-packages (from akshare) (0.82.2)\n",
      "Requirement already satisfied: tabulate>=0.8.6 in c:\\anaconda\\lib\\site-packages (from akshare) (0.9.0)\n",
      "Requirement already satisfied: decorator>=4.4.2 in c:\\anaconda\\lib\\site-packages (from akshare) (5.1.1)\n",
      "Requirement already satisfied: nest_asyncio>=1.6.0 in c:\\anaconda\\lib\\site-packages (from akshare) (1.6.0)\n",
      "Requirement already satisfied: mini-racer>=0.12.4 in c:\\anaconda\\lib\\site-packages (from akshare) (0.12.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\anaconda\\lib\\site-packages (from aiohttp>=3.11.13->akshare) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\anaconda\\lib\\site-packages (from aiohttp>=3.11.13->akshare) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\anaconda\\lib\\site-packages (from aiohttp>=3.11.13->akshare) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\anaconda\\lib\\site-packages (from aiohttp>=3.11.13->akshare) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\anaconda\\lib\\site-packages (from aiohttp>=3.11.13->akshare) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\anaconda\\lib\\site-packages (from aiohttp>=3.11.13->akshare) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\anaconda\\lib\\site-packages (from aiohttp>=3.11.13->akshare) (1.18.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4>=4.9.1->akshare) (2.5)\n",
      "Requirement already satisfied: six>=1.9 in c:\\anaconda\\lib\\site-packages (from html5lib>=1.0.1->akshare) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\anaconda\\lib\\site-packages (from html5lib>=1.0.1->akshare) (0.5.1)\n",
      "Requirement already satisfied: et_xmlfile in c:\\anaconda\\lib\\site-packages (from openpyxl>=3.0.3->akshare) (1.1.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\anaconda\\lib\\site-packages (from pandas>=0.25->akshare) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda\\lib\\site-packages (from pandas>=0.25->akshare) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda\\lib\\site-packages (from pandas>=0.25->akshare) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\anaconda\\lib\\site-packages (from pandas>=0.25->akshare) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests>=2.22.0->akshare) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests>=2.22.0->akshare) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests>=2.22.0->akshare) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm>=4.43.0->akshare) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install akshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\刘其睿\\Desktop\\安智云台\\BPW\\code\\SearchModule250319\n",
      ".env file exists: True\n",
      "The capital of France is **Paris**. It is renowned for its rich history, iconic landmarks such as the Eiffel Tower, and its cultural significance.\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "env_file_path = \".env\"\n",
    "print(f\".env file exists: {os.path.exists(env_file_path)}\")\n",
    "\n",
    "load_dotenv(dotenv_path=env_file_path, override=True)\n",
    "default_ds_settings = {\n",
    "    \"model\": \"deepseek-chat\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 8192,\n",
    "    \"top_p\": 0.9,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"presence_penalty\": 0,\n",
    "}\n",
    "def query_ds(sys_prompt, user_prompt,settings=default_ds_settings):\n",
    "    test_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "    test_api_base = os.getenv(\"DEEPSEEK_API_BASE\")\n",
    "    client = OpenAI(api_key=test_api_key, base_url=test_api_base)\n",
    "    if \"deepseek-reasoner\" in settings[\"model\"]:\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            **settings,\n",
    "            stream=False,\n",
    "        )\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            **settings,\n",
    "            stream=False,\n",
    "            logprobs=True,\n",
    "            top_logprobs=5\n",
    "        )\n",
    "    return response, int(response.usage.total_tokens)\n",
    "\n",
    "sys_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the capital of France?\"\n",
    "# response, tokens = query_ds(sys_prompt, user_prompt, settings=settings)\n",
    "response, tokens = query_ds(sys_prompt, user_prompt, settings=default_ds_settings)\n",
    "print(response.choices[0].message.content)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def analysis_fenbi(fenbi_data1, fenbi_data2, fenbi_data3):\n",
    "    try:\n",
    "        # 构造 Deepseek 提示词\n",
    "        prompt = f\"\"\"\n",
    "        # Role: 主力动向分析师\n",
    "        \n",
    "        ## Profile\n",
    "        - language: 中文\n",
    "        - description: 主力动向分析师专注于通过分析集合竞价时间的资金明细，识别和预测主力资金的动向，帮助投资者做出更明智的投资决策。\n",
    "        - background: 拥有金融分析、数据科学和人工智能的背景，专注于股票市场的主力资金分析。\n",
    "        - personality: 严谨、细致、逻辑性强\n",
    "        - expertise: 金融数据分析、主力资金动向预测、股票市场分析\n",
    "        - target_audience: 股票投资者、金融分析师、投资机构\n",
    "        \n",
    "        ## Skills\n",
    "        1. 数据分析\n",
    "           - 数据清洗: 能够处理和分析大量的历史分笔数据，确保数据的准确性和完整性。\n",
    "           - 模式识别: 能够识别主力资金的典型操作模式，如大单买入或卖出。\n",
    "           - 趋势预测: 基于历史数据，预测股票的未来走势。\n",
    "           - 异常检测: 能够检测数据中的异常点，如异常的大单交易。\n",
    "        2. 金融知识\n",
    "           - 股票市场分析: 深入理解股票市场的运作机制和影响因素。\n",
    "           - 主力资金行为分析: 熟悉主力资金的常见操作手法和策略。\n",
    "           - 投资策略建议: 能够根据分析结果，提供具体的投资策略建议。\n",
    "        \n",
    "        ## Rules\n",
    "        1. 基本原则：\n",
    "           - 数据驱动: 所有分析和结论必须基于提供的数据，禁止编造或假设数据。\n",
    "           - 客观公正: 分析过程中保持客观，不受个人情感或外部因素影响。\n",
    "           - 透明性: 分析方法和过程必须透明，便于验证和复现。\n",
    "           - 及时性: 分析结果应及时提供，确保信息的时效性。\n",
    "        2. 行为准则：\n",
    "           - 保密性: 严格保护用户提供的数据，不泄露任何敏感信息。\n",
    "           - 专业性: 保持专业态度，提供高质量的分析服务。\n",
    "           - 用户导向: 以用户需求为中心，提供有针对性的分析建议。\n",
    "           - 持续学习: 不断更新知识和技能，适应市场变化。\n",
    "        3. 限制条件：\n",
    "           - 数据限制: 分析结果受限于提供的数据质量和数量。\n",
    "           - 市场风险: 股票市场存在不确定性，分析结果仅供参考。\n",
    "           - 时间限制: 分析过程可能需要一定时间，用户需耐心等待。\n",
    "           - 技术限制: 分析工具和方法可能存在技术限制，影响分析结果。\n",
    "        \n",
    "        ## Workflows\n",
    "        - 目标: 分析个股的历史分笔数据，识别主力资金的动向，预测未来走势。\n",
    "        - 步骤 1: 数据清洗和预处理，确保数据的准确性和完整性。\n",
    "        - 步骤 2: 分析数据中的大单交易，识别主力资金的典型操作模式。\n",
    "        - 步骤 3: 基于识别出的模式，预测股票的未来走势。\n",
    "        - 预期结果: 提供详细的分析报告，包括主力资金的操作手法和未来走势预测。\n",
    "        \n",
    "        ## Initialization\n",
    "        作为主力动向分析师，你必须遵守上述Rules，按照Workflows执行任务。\n",
    "        ---\n",
    "        以下是个股的历史分笔数据，这是三天的数据{fenbi_data1},{fenbi_data2},{fenbi_data3}，你帮我分析一下这只股票是否存在主力操盘的行为，如果是，那么主力是如何操作的，以及接下来的走势如何？必须根据投喂你的数据进行分析，禁止自己编造假数据。描述当天的股价走势，自证没有虚构数据。你的观点需要数据支撑。\n",
    "        \"\"\"\n",
    "        \n",
    "        sys_prompt = \"\"\n",
    "        response, tokens = query_ds(sys_prompt, user_prompt=prompt)\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"AI 分析生成失败: {str(e)}\"\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3f09fdf0c94e8ea79f74a739c6aec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725f4bff3d244ec89892e769f6d2e804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57ffa6bedc74b4f8db9af148000a34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import akshare as ak\n",
    "Test_fenbi_data1 = ak.stock_intraday_sina(symbol=\"sh600588\", date=\"20250318\")\n",
    "Test_fenbi_data2 = ak.stock_intraday_sina(symbol=\"sh600588\", date=\"20250319\")\n",
    "Test_fenbi_data3 = ak.stock_intraday_sina(symbol=\"sh600588\", date=\"20250320\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = analysis_fenbi(Test_fenbi_data1, Test_fenbi_data2, Test_fenbi_data3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 数据分析与主力动向识别\\n\\n#### 1. 数据清洗与预处理\\n首先，我们对三天的历史分笔数据进行清洗和预处理，确保数据的准确性和完整性。数据包括以下字段：\\n- `symbol`: 股票代码\\n- `name`: 股票名称\\n- `ticktime`: 交易时间\\n- `price`: 交易价格\\n- `volume`: 交易量\\n- `prev_price`: 前一交易价格\\n- `kind`: 交易类型（U: 主动买入，D: 主动卖出，E: 中性交易）\\n\\n#### 2. 大单交易分析\\n主力资金通常通过大单交易来影响股价。我们通过以下步骤识别大单交易：\\n- **定义大单**：将单笔交易量超过一定阈值的交易定义为大单。根据数据分布，我们设定阈值为 **50,000 股**。\\n- **识别大单**：筛选出所有单笔交易量超过 50,000 股的交易。\\n\\n#### 3. 主力资金操作模式识别\\n通过分析大单交易的时间和价格变化，识别主力资金的操作模式：\\n- **第一天**：\\n  - 开盘时出现一笔 **272,500 股** 的大单买入（09:25:00），价格从 17.10 元上涨至 17.20 元。\\n  - 随后在 09:30:07 和 09:30:10 分别出现 **169,900 股** 和 **113,100 股** 的大单买入，价格稳定在 17.20 元。\\n  - 尾盘出现一笔 **1,266,700 股** 的大单买入（15:00:03），价格从 17.14 元上涨至 17.16 元。\\n  - **结论**：主力资金在开盘和尾盘通过大单买入推高股价，显示出明显的吸筹行为。\\n\\n- **第二天**：\\n  - 开盘时出现一笔 **335,500 股** 的大单买入（09:25:01），价格从 17.14 元下跌至 17.02 元，可能是主力资金压低股价吸筹。\\n  - 随后在 09:30:06 和 09:30:09 分别出现 **82,300 股** 和 **71,100 股** 的大单买入，价格回升至 17.05 元。\\n  - 尾盘出现一笔 **1,204,951 股** 的大单买入（15:00:03），价格从 16.60 元上涨至 16.61 元。\\n  - **结论**：主力资金在开盘压低股价吸筹，尾盘通过大单买入推高股价，显示出明显的控盘行为。\\n\\n- **第三天**：\\n  - 开盘时出现一笔 **229,900 股** 的大单买入（09:25:00），价格从 16.61 元下跌至 16.58 元，可能是主力资金压低股价吸筹。\\n  - 随后在 09:30:07 和 09:30:10 分别出现 **64,800 股** 和 **51,400 股** 的大单买入，价格回升至 16.57 元。\\n  - 尾盘出现一笔 **748,200 股** 的大单买入（15:00:00），价格从 16.52 元上涨至 16.51 元。\\n  - **结论**：主力资金在开盘压低股价吸筹，尾盘通过大单买入推高股价，显示出明显的控盘行为。\\n\\n#### 4. 主力资金操作手法总结\\n- **吸筹阶段**：主力资金在开盘时通过大单买入或卖出压低股价，吸引散户卖出，同时吸筹。\\n- **控盘阶段**：主力资金在盘中通过大单买入推高股价，维持股价稳定。\\n- **尾盘拉升**：主力资金在尾盘通过大单买入推高股价，制造上涨趋势。\\n\\n#### 5. 未来走势预测\\n- **短期走势**：主力资金在三天内通过大单买入推高股价，显示出明显的控盘行为。预计短期内股价将继续上涨，但需警惕主力资金在高位出货。\\n- **中长期走势**：如果主力资金继续吸筹并控盘，股价有望进一步上涨。但如果主力资金在高位出货，股价可能出现回调。\\n\\n### 数据支撑\\n- **第一天**：\\n  - 09:25:00：272,500 股大单买入，价格从 17.10 元上涨至 17.20 元。\\n  - 15:00:03：1,266,700 股大单买入，价格从 17.14 元上涨至 17.16 元。\\n- **第二天**：\\n  - 09:25:01：335,500 股大单买入，价格从 17.14 元下跌至 17.02 元。\\n  - 15:00:03：1,204,951 股大单买入，价格从 16.60 元上涨至 16.61 元。\\n- **第三天**：\\n  - 09:25:00：229,900 股大单买入，价格从 16.61 元下跌至 16.58 元。\\n  - 15:00:00：748,200 股大单买入，价格从 16.52 元上涨至 16.51 元。\\n\\n### 结论\\n根据数据分析，用友网络（sh600588）在三天内存在明显的主力资金操盘行为。主力资金通过大单买入推高股价，显示出吸筹和控盘的迹象。预计短期内股价将继续上涨，但需警惕主力资金在高位出货的风险。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 数据分析与主力动向识别\n",
       "\n",
       "#### 1. 数据清洗与预处理\n",
       "首先，我们对三天的历史分笔数据进行清洗和预处理，确保数据的准确性和完整性。数据包括以下字段：\n",
       "- `symbol`: 股票代码\n",
       "- `name`: 股票名称\n",
       "- `ticktime`: 交易时间\n",
       "- `price`: 交易价格\n",
       "- `volume`: 交易量\n",
       "- `prev_price`: 前一交易价格\n",
       "- `kind`: 交易类型（U: 主动买入，D: 主动卖出，E: 中性交易）\n",
       "\n",
       "#### 2. 大单交易分析\n",
       "主力资金通常通过大单交易来影响股价。我们通过以下步骤识别大单交易：\n",
       "- **定义大单**：将单笔交易量超过一定阈值的交易定义为大单。根据数据分布，我们设定阈值为 **50,000 股**。\n",
       "- **识别大单**：筛选出所有单笔交易量超过 50,000 股的交易。\n",
       "\n",
       "#### 3. 主力资金操作模式识别\n",
       "通过分析大单交易的时间和价格变化，识别主力资金的操作模式：\n",
       "- **第一天**：\n",
       "  - 开盘时出现一笔 **272,500 股** 的大单买入（09:25:00），价格从 17.10 元上涨至 17.20 元。\n",
       "  - 随后在 09:30:07 和 09:30:10 分别出现 **169,900 股** 和 **113,100 股** 的大单买入，价格稳定在 17.20 元。\n",
       "  - 尾盘出现一笔 **1,266,700 股** 的大单买入（15:00:03），价格从 17.14 元上涨至 17.16 元。\n",
       "  - **结论**：主力资金在开盘和尾盘通过大单买入推高股价，显示出明显的吸筹行为。\n",
       "\n",
       "- **第二天**：\n",
       "  - 开盘时出现一笔 **335,500 股** 的大单买入（09:25:01），价格从 17.14 元下跌至 17.02 元，可能是主力资金压低股价吸筹。\n",
       "  - 随后在 09:30:06 和 09:30:09 分别出现 **82,300 股** 和 **71,100 股** 的大单买入，价格回升至 17.05 元。\n",
       "  - 尾盘出现一笔 **1,204,951 股** 的大单买入（15:00:03），价格从 16.60 元上涨至 16.61 元。\n",
       "  - **结论**：主力资金在开盘压低股价吸筹，尾盘通过大单买入推高股价，显示出明显的控盘行为。\n",
       "\n",
       "- **第三天**：\n",
       "  - 开盘时出现一笔 **229,900 股** 的大单买入（09:25:00），价格从 16.61 元下跌至 16.58 元，可能是主力资金压低股价吸筹。\n",
       "  - 随后在 09:30:07 和 09:30:10 分别出现 **64,800 股** 和 **51,400 股** 的大单买入，价格回升至 16.57 元。\n",
       "  - 尾盘出现一笔 **748,200 股** 的大单买入（15:00:00），价格从 16.52 元上涨至 16.51 元。\n",
       "  - **结论**：主力资金在开盘压低股价吸筹，尾盘通过大单买入推高股价，显示出明显的控盘行为。\n",
       "\n",
       "#### 4. 主力资金操作手法总结\n",
       "- **吸筹阶段**：主力资金在开盘时通过大单买入或卖出压低股价，吸引散户卖出，同时吸筹。\n",
       "- **控盘阶段**：主力资金在盘中通过大单买入推高股价，维持股价稳定。\n",
       "- **尾盘拉升**：主力资金在尾盘通过大单买入推高股价，制造上涨趋势。\n",
       "\n",
       "#### 5. 未来走势预测\n",
       "- **短期走势**：主力资金在三天内通过大单买入推高股价，显示出明显的控盘行为。预计短期内股价将继续上涨，但需警惕主力资金在高位出货。\n",
       "- **中长期走势**：如果主力资金继续吸筹并控盘，股价有望进一步上涨。但如果主力资金在高位出货，股价可能出现回调。\n",
       "\n",
       "### 数据支撑\n",
       "- **第一天**：\n",
       "  - 09:25:00：272,500 股大单买入，价格从 17.10 元上涨至 17.20 元。\n",
       "  - 15:00:03：1,266,700 股大单买入，价格从 17.14 元上涨至 17.16 元。\n",
       "- **第二天**：\n",
       "  - 09:25:01：335,500 股大单买入，价格从 17.14 元下跌至 17.02 元。\n",
       "  - 15:00:03：1,204,951 股大单买入，价格从 16.60 元上涨至 16.61 元。\n",
       "- **第三天**：\n",
       "  - 09:25:00：229,900 股大单买入，价格从 16.61 元下跌至 16.58 元。\n",
       "  - 15:00:00：748,200 股大单买入，价格从 16.52 元上涨至 16.51 元。\n",
       "\n",
       "### 结论\n",
       "根据数据分析，用友网络（sh600588）在三天内存在明显的主力资金操盘行为。主力资金通过大单买入推高股价，显示出吸筹和控盘的迹象。预计短期内股价将继续上涨，但需警惕主力资金在高位出货的风险。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(report.choices[0].message.content.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
